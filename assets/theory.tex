\chapter{Theoretical Background}

\lettrine[
	nindent=0em, findent=0.5em, loversize=-0.12, lines=5
]{\initfamily{D}}{\bfseries\color{Blue}eep learning}\index{Deep learning},
\emph{a class of \gls{ml}\index{Machine learning} algorithms based on
\glspl{nn}}, has revolutionized the way we tackle a problem from a \gls{ml}
perspective and is one if not the most important factor for recent \gls{ml}
achievements. Solving complex tasks such image classification\index{Image
classification} or language translation\index{Language translation}, that for
years have bedevilled traditional \gls{ml} algorithms, constitutes the signature
of \gls{dl}. Admittedly, \emph{the advent of a deep
\gls{cnn}\index{Convolutional neural network}}, the AlexNet \parencite{alexnet}
on September 30 of 2012, signified the ``modern birthday'' of this field. On
this day, AlexNet\index{AlexNet} not only won the ImageNet \parencite{Deng_2009}
\gls{ilsvrc}, but dominated it, achieving a top-5 accuracy\index{Top-5 accuracy}
of \SI{85}{\percent}, surpassing the runner-up which achieved a top-5 accuracy
of \SI{75}{\percent}.  AlexNet showed that \glspl{nn} are not merely a
pipe-dream, but they can be applied in real-world problems. It is worth to
notice that ideas of \glspl{nn} trace back to 1943, but it was until recently
that these ideas got materialized. The reason for this recent breakthrough of
\gls{dl} (and \gls{ml}) is twofold. First, the availability of large
datasets\index{Dataset}---the era of big data\index{Big data}---such as
ImageNet\index{ImageNet}. Second, the increase in computational power, mainly of
\acrshortpl{gpu} for \gls{dl}, accelerating the training of deep \glspl{nn} and
traditional \gls{ml} algorithms.

\section{Machine Learning Preliminaries}

Since \gls{dl} is a subfield of \gls{ml}, it is necessary to familiarize with
the later before diving into the former. In this section, the minimum
theoretical background and jargon of \gls{ml} is presented. \Acrlong{ml} can be
defined as \emph{``the science and (art) of programming computers so they can
learn from the data''} \parencite{ml}. A more technical definition is the following:

\begin{definition}[name={Machine learning, \cite{mitchell}}]
	A computer program is said to learn from experience\index{Experience}
	\textbf{E} with respect to some class of tasks\index{Task} \textbf{T} and some
	performance measure \textbf{P}\index{Performance measure}, if its
	performance on \textbf{T}, as measured by \textbf{P}, improves with
	experience \textbf{E}.
\end{definition}

For instance, a computer program\index{Spam filter} that classifies emails into
spam and non-spam (the task \textbf{T}), can improve its accuracy, i.e. the
percentage of correctly classified emails (the performance \textbf{P}), through
examples of spam and non-spam emails (the experience \textbf{E}). But in order
to take advantage of the experience aka \emph{data}\index{Data}, it must be
written in such a way that \emph{adapts to the patterns in the data}. Certainly,
a \emph{traditional spam filter can not learn from experience}, since the latter
does not affect the classification rules of the former and as such, its
performance. For a traditional spam filter to adapt to new patterns and perform
better, it must change its hard-wired rules, but by then it will be a different
program. In contrast, a \emph{\gls{ml}-based filter can adapt to new patterns,
simply because it has been programmed to do so}. In other words, \emph{in
traditional programming\index{Traditional programming} we write rules for
solving \textbf{T} whereas \textbf{in \gls{ml} we write rules to learn the
rules} for solving \textbf{T}}. This subtle but essential difference is what
gives \gls{ml} algorithms the ability to take advantage of the data.

\subsection{Learning paradigms}
\label{subsec:paradigms}

Depending on the type of experience they are allowed to have during their
\emph{training phase}\index{Training phase} \parencite{deeplearning}, \gls{ml}
approaches are divided into three main \emph{learning paradigms}\index{Learning
paradigms}: \emph{\textbf{unsupervised learning}\index{Unsupervised learning},
\textbf{supervised learning}\index{Supervised learning} and
\textbf{reinforcement learning}\index{Reinforcement learning}}. The following
definitions are not by any means formal, but merely serve as an intuitive
description of the different paradigms.

\begin{definition}[name=Unsupervised learning]
	The experience comes in the form $\dtrain = \{\vcx_i\}$, where $\vcx_i \sim
	p(\vcx)$ is the input\index{Input} of the $i$-th training
	instance\index{Training instance} aka sample\index{Training sample}. In this
	paradigm we are interested in learning useful properties of the underlying
	structure captured by $p(\vcx)$ or $p(\vcx)$ itself.
\end{definition}

For example, suppose we are interested in generating images that look like
Picasso paintings. In this case, the input is just the pixel values, i.e.  $\vcx
\in \mathbb{R}^{W \times H \times 3}$. The latter follow a distribution
$p(\vcx)$, so all we have to do is to train an unsupervised learning algorithm
with many Picasso paintings to get a \emph{model}, that is $\hat{p}(\vcx)$.
Assuming the estimation of the original distribution is good enough, new
realistically looking paintings (with respect to original Picasso paintings) can
be ``drawn'' by just sampling from $\hat{p}(\vcx)$. In the ML parlance, this
task is known as \emph{generative modeling}\index{Generative modeling} while
inputs are also called \emph{features, predictors or
descriptors}\index{Feature}\index{Descriptor}\index{Predictor}.

\begin{definition}[name=Supervised learning]
	The experience comes in the form $\dtrain = \{(\vcx_i, \vcy_i)\}$, where
	$(\vcx_i, \vcy_i) \sim p(\vcx, \vcy)$ and $\vcy_i$ is the
	output\index{Output} aka label\index{Label} of the $i$-th training instance.
	In this paradigm we are usually interested in learning a function $f \colon
	\mcl{X} \to \mcl{Y}$.
\end{definition}

This paradigm comes mainly under two flavors: \emph{regression\index{Regression}
and classification\index{Classification}}, which are schematically depicted in
\Figure{} \ref{fig:supervised_learning}. In regression the interest is in
predicting a continuous value given an input, i.e.  $y \in \mathbb{R}$, such as
a molecular property given a mathematical representation of a molecule. In
classification, the interest is to predict in which of $k$ classes an input
belongs to, i.e. $y \in \{1, \ldots, k\}$, such as predicting the breed of a dog
image given the raw pixel values of the image. The term ``supervised'' is coined
due to the ``human supervision'' the algorithm receives during its training
phase\index{Training phase}, through the presence of the correct answer (the
label) in the experience.  In a sense, in this paradigm we ``teach'' the
learning algorithm aka \emph{learner}. It should be emphasized that the label is
not constrained to be single-valued, but can also be multi-valued. In this case,
one talks about \emph{multi-label regression\index{Multi-label regression} or
classification\index{Multi-label classification}} \parencite{Read_2009}.

A more exotic form of supervised learning is \emph{conditional generative
modelling}\index{Conditional generative modelling}, where the interest is in
estimating $p(\vcx \mid \vcy)$. For example, one may want to build a model that
generates images of a specific category or a \emph{model that designs
molecules/materials with tailored properties} \parencite{Kim2018, Yao2021,
Gebauer2022}. This is one approach of how \gls{ml} can tackle the \emph{inverse
design problem}\index{Inverse design} in chemistry\index{Chemistry}.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\begin{tikzpicture}
			\pgfmathsetseed{1}
			\begin{axis}[
					thick, ticks=none, mark size=1.2pt,
					xmin=0, xmax=1.1, ymin=0, ymax=1,
					xlabel=$x_1$, ylabel=$x_2$,
				]
				\coordinate (start) at (0.62, 0);
				\coordinate (stop) at (0.2, 1);
				\coordinate (xmax-ymin) at (1.1, 0);
				\coordinate (xmin-ymin) at (0, 0);
				\coordinate (xmin-ymax) at (0, 1);
				\coordinate (xmax-ymax) at (1.1, 1);
				\addplot+ [
					only marks, samples=20, variable=\t, domain=0:1,
				] ({exp(-0.55*t^2) + 0.01*rnd}, {0.6*rnd});
				\addplot+ [
					only marks, samples=20, variable=\t, domain=0.4:1, mark=*,
				] ({exp(-5*t^2) + 0.05*rnd}, {0.8*rnd});
				% y = -2.381x + 1.476
				\addplot [samples=2, black, dashed, ultra thick] {-2.381*x + 1.476};
				% y = -3.381x + 1.476
				\addplot [samples=2, black, ultra thick] {-5.581*x + 2.976};
				\legend{,,Model, True $f$};
			\end{axis}
		\end{tikzpicture}
		\caption{Classification.}
		\label{fig:classification}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\begin{tikzpicture}
			\centering
			\pgfmathsetseed{1}
			\begin{axis}[
					thick, xlabel=$x_{\phantom{1}}$, ylabel=$y$, domain=-2:2,
					legend pos=north west, ticks=none,
				]
				\addplot [
					only marks, samples=12,
					mark options={fill=blue!20, mark size=2.5pt,}
				] {x^2 + x^3 + 2*rnd};
				\addplot [no marks, dashed, ultra thick, samples=20] {x^2 + 0.1*x^3 + 0.1*x};
				% The true function
				\addplot [no marks, ultra thick, samples=20] {x^2 + x^3 + 0.5};
				\legend{,Model, True $f$};
			\end{axis}
		\end{tikzpicture}
		\caption{Regression.}
	\end{subfigure}
	\caption{Main tasks of supervised learning.}
	\label{fig:supervised_learning}
\end{figure}

\begin{definition}[Reinforcement learning]
	The experience comes from the interaction of the learner, called
	agent\index{Agent} in this context, with its environment. In other words,
	there is a feedback loop between the learner and its environment. In this
	paradigm we are interested in building an agent that can take suitable
	actions in a given situation.
\end{definition}

The agent observes its \emph{environment}, selects and performs \emph{actions}
and gets \emph{rewards or penalties} in return. The goal is to learn an optimal
strategy, called a \emph{policy}, that \emph{maximizes the long-term reward}
\parencite{ml}. A policy simply defines the action that the agent should
choose in a given situation. In contrast to supervised learning, where the
correct answers are provided to the learner, \emph{in reinforcement learning the
learner must find the optimal answers by trial and error}
\parencite{bishop2007}. Reinforcement learning techniques find application in
fields such as gaming (AlphaGo is a well known example), robotics, autonomous
driving and recently chemistry \parencite{li, Gow2022}. Since in the present
thesis only supervised learning techniques were employed, the remaining of this
chapter is devoted to this learning paradigm.

\subsection{Formulating the problem of supervised learning}
\label{subsec:supervised_learning}

The general setting of supervised learning is as follows: we assume that there
is some relationship between $\vcx$ and $\vcy$:
\begin{equation}
	\label{eq:supervised}
	\vcy = f(\vcx) + \vc{\epsilon}
\end{equation}
and we want to estimate $f$ from the data. The function $f$ represents the
\emph{systematic information} that $\vcx$ gives about $\vcy$ while
$\vc{\epsilon}$ is a random \emph{error term}\index{Error term} independent of
$\vcx$ and with zero mean. More formally, we have an input space $X$, an output
space $Y$ and we are interested in learning a function $\hat{h} \colon \mcl{X}
\to \mcl{Y}$, called the \emph{hypothesis}\index{Hypothesis}, which produces an
output $\vcy \in \mcl{Y}$ given an input $\vcx \in \mcl{X}$. At our disposal we have a
collection of input-output pairs $(\vcx_i, \vcy_i)$, forming the
\emph{\textbf{training set}}\index{Training set} $\dtrain$, with the pairs drawn
\acrshort{iid} from $p(\vcx, \vcy)$.

Ideally, we would like to learn a hypothesis that minimizes the
\emph{\textbf{generalization error or loss}}\index{Generalization
error}\index{Generalization loss}:
\begin{equation}
	\label{eq:generalization_loss}
	\loss \coloneqq
	\int_{\mcl{X} \times \mcl{Y}} \ell(h(\vcx), \vcy) p(\vcx, \vcy) d\vcx d\vcy
\end{equation}
that is, the expected value of some \emph{loss function}\index{Loss function}
$\ell$ over all possible input-output pairs. A loss function just measures the
discrepancy of the prediction $h(\vcx) = \hat{\vcy}$ from the true value $\vcy$
and as such, the best hypothesis is the one that minimizes this integral.
Obviously, it is impossible to evaluate the integral in \Equation{}
\ref{eq:generalization_loss}, since we don't have access to infinite data.

The idea is to use the \emph{training error or loss}\index{Training
error}\index{Training loss}:
\begin{equation}
	\label{eq:training_loss}
	\loss_\text{train} \coloneqq \frac{1}{\lvert \dtrain \rvert} \sum_{i \in \dtrain}
	\ell(h(\vcx_i), \vcy_i)
\end{equation}
as an approximation for the generalization loss, and \emph{choose the hypothesis
that minimizes the training loss}, a principle known as \emph{empirical risk
minimization}\index{Empirical risk minimization}. In other words, to get a
hypothesis aka \emph{model} $\mcl{M}$\index{Model} from the data, we need to
solve the following optimization problem:
\begin{equation}
	\label{eq:problem}
	\hat{h} \gets \argmin_{h \in \mcl{H}} \ltrain
\end{equation}
which is achieved by just feeding the training data into the learning
algorithm $\mcl{A}$\index{Learning algorithm}:
\begin{equation}
	\label{eq:learner}
	\mcl{M} \gets \mcl{A}(\dtrain)
\end{equation}

\subsection{Components of a learning algorithm}

By breaking down \Equation{} \ref{eq:problem}, i.e. the optimization problem the
learner needs to solve, the components of a learner can be revealed. The latter
is comprised of the following three ``orthogonal'' components: \emph{a
\textbf{hypothesis space}\index{Hypothesis space}, a \textbf{loss
function} and an \textbf{optimizer}}. We now look into each of them
individually and describe the contribution of each one to the solution of the
optimization problem. For the ease of notation and clarity, in the remaining of
this chapter we will stick to examples from simple (single-valued) regression
and binary classification.

\begin{definition}[Hypothesis space]
	The set of hypotheses (functions), denoted as $\mcl{H}$, from which the
	learner is allowed to pick the solution of \Equation{} \ref{eq:problem}.
\end{definition}

A simple example of a hypothesis space, is the one used in univariate
\emph{linear regression}\index{Linear regression}:
\begin{equation}
	\label{eq:linear}
	\hat{y} = \beta_0 + \beta x
\end{equation}
where $\mcl{H}$ contains all lines (or hyperplanes in the multivariate case)
defined by \Equation{} \ref{eq:linear}. Of course, one can get \emph{a
more expressive} hypothesis space, by including polynomial terms, e.g.:
\begin{equation}
	\label{eq:polynomial}
	\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2
\end{equation}
The more expressive the hypothesis space, the larger the
\emph{\textbf{representational capacity}}\index{Representational capacity} of
the learning algorithm. For a formal definition of representational capacity,
the interested reader can look at \emph{Vapnik-Chervonenkis
Dimension}\index{Vapnik-Chervonenkis dimension} \parencite{Hastie2009}.

\begin{definition}[Loss function]
	A function that maps a prediction into a real number, which intuitively
	represents the quality of a candidate hypothesis.
\end{definition}

For example, a typical loss function used in regression is the \emph{squared
loss}\index{Squared loss}:
\begin{equation}
	\label{eq:squared_loss}
	\ell(\hat{y}, y) \coloneqq (\hat{y} - y)^2
\end{equation}
where $y, \hat{y} \in \mathbb{R}$. A typical loss function for binary
classification is the \emph{binary cross entropy
loss}\index{Binary cross entropy loss}:
\begin{equation}
	\label{eq:cross_entropy}
	\ell(\hat{y}, y) \coloneqq y \cdot \log (\hat{y})
	+ (1 - y) \cdot \log (1 - \hat{y})
\end{equation}
where $y \in \{0, 1\}$, indicating the correct class, and $\hat{y} \in [0, 1]$
which corresponds to the predicted probability for class-$1$. Notice that in
both cases the loss is minimum when the prediction is equal to the ground truth.
For the cross entropy loss, if $y=1$ is the correct class, then the model must
predict $\hat{y}=1$ for the loss to be minimized.

Usually, we are not only penalizing a hypothesis for its mispredictions, but
also for its \emph{complexity}. This is done in purpose, since a learner with a
\emph{very rich hypothesis space} can easily memorize the training set but fail
to generalize well to new unseen examples. \emph{Every modification that is made
to a learner in order to reduce its generalization loss but not its training
loss, is called \textbf{regularization}}\index{Regularization} \parencite{deeplearning}.

A common---but not the only---way to achieve that, is by including another
penalty term called \emph{regularization term or
regularizer}\index{Regularizer}, denoted as $\mcl{R}$, in \Equation{}
\ref{eq:training_loss}:
\begin{equation}
	\label{eq:regularization}
	\loss_\text{train} \coloneqq \frac{1}{\lvert \dtrain \rvert} \sum_{i \in \dtrain}
	\ell(h(\vcx_i), \vcy_i)
	+
	\lambda \mcl{R}
\end{equation}
The $\lambda$ factor controls the strength of regularization and it is an
\emph{\textbf{hyperparameter}}\index{Hyperparameter}, i.e. a parameter that is
not learned during training but \emph{whose value is used to control the
training phase\index{Training phase}.} In order to see how $\lambda$ penalizes
model complexity, assume we perform univariate polynomial
regression\index{Polynomial regression} of degree $k$:
\begin{equation}
	\hat{y} = \beta_0 + \sum_{i=1}^k \beta_i x^i
\end{equation}
combining \gls{msl}\index{Mean squared loss} and \emph{Lasso
regularization}\index{Lasso} as training loss:
\begin{equation}
	\label{eq:lasso}
	\loss_\text{train} = \frac{1}{\lvert \dtrain \rvert} \sum_{i \in \dtrain}
	\ell(\hat{y}_i - y_i)^2
	+
	\lambda \sum_{i=1}^k \lvert \beta_i \rvert
\end{equation}
Lets apply a very strong regularization by setting $\lambda \to \infty$ (in
practice we set $\lambda$ to a very large value) and observe what happens to the
\emph{weights}\index{Weights} $\beta_i$. By setting $\lambda \to
\infty$, the regularization term dominates the \gls{msl} and as such, the only
way to minimize the training loss is by setting $\beta_i = 0$. This leave us
with a very simple model---only the \emph{bias}\index{Bias} $\beta_0$
survives---which always predicts the mean value of $y$ in the training set.

Applying a regularizer\index{Regularizer}, is also useful when we need to select
between two (or more) competing hypotheses that are equally good. For example,
assuming two hypotheses achieve the same (unregularized) training loss, the
inclusion of a regularization term help us decide between the two, \emph{by
favoring the simplest one}. This is reminiscent of the \emph{\textbf{Occam's
razor} aka \textbf{principle of parsimony}}\index{Occam's razor}\index{Principle
of parsimony}, which advocates that between two competing theories with equal
explanatory power, one should prefer the one with the fewest assumptions.

\begin{definition}[Optimizer]
	An algorithm that searches through $\mcl{H}$ for the solution of \Equation{}
	\ref{eq:problem}.
\end{definition}

Having defined the set of candidate models (the hypothesis space) and a measure
that quantifies the quality of a given model (the loss function), all that is
remaining is a tool to scan the hypothesis space and pick the model that
minimizes the training loss (the optimizer). A naive approach is to check all
hypotheses in $\mcl{H}$ and then pick the one that achieves the lowest training
loss. This approach can work if $\mcl{H}$ is finite, but obviously doesn't scale
in the general case where $\mcl{H}$ is infinite\footnote{It is not
uncommon for $\mcl{H}$ to be infinite. Even for simple learners like linear
regression $\mcl{H}$ is infinite, since there infinite lines defined by \Equation{}
\ref{eq:linear}.}. More efficient approaches are needed if we are aiming to solve
\Equation{} \ref{eq:problem} in finite time.

One optimizer that is frequently used in \gls{ml} and is the precursor of more
refined ones, is \emph{\textbf{gradient descent}}\index{Gradient descent}. With
this method, the exploration of hypothesis space\footnote{We have implicitly
assumed that $\mcl{H}$ can be parameterized, i.e. $\mcl{H} = \{h(\vcx;\vcth)
\mid \vcth \in \Theta\}$, where $\Theta$ denotes the parameter space, the set of
all values the parameter $\vcth$ can take. This allows us to write the training
loss as function of model parameters and optimize them with gradient descent.}
involves the following steps:

\begin{algorithm}[H]
	$\vcth \gets$ random initialization\;
	\While{stopping criterion not met}{
		$\vcth \gets \vcth - \eta \nabla_{\vcth}\ltrain(\vcth)$\; 
	}
	\caption{Gradient descent}
	\label{algo:gd}
\end{algorithm}

\noindent where $\eta$ is a small number called the \emph{learning
rate}\index{Learning rate}. Gradient descent is based on the idea that
if a multivariate function is defined and differentiable at a point $\vcx$, then
$f(\vcx)$ \emph{decreases fastest if one takes a small step from $\vcx$ in the
direction of negative gradient at $\vcx$, $-\nabla f(\vcx)$}.

The motivation becomes clear if we look at the differential of $f(\vcx)$ in
direction $\vc{u}$: \begin{equation} \label{eq:differential} f(\vcx + \delta
\vc{u}) - f(\vcx) = \nabla f(\vcx) \cdot \delta \vc{u} \end{equation} \Equation{}
\ref{eq:differential} says that this differential is minimized\footnote{The
right hand side of \Equation{} \ref{eq:differential} is a dot product.} when
$\delta \vc{u}$ is anti-parallel to $\nabla f(\vcx)$ and that is why we subtract
the gradient in \Algorithm{} \ref{algo:gd}, i.e. move in direction anti-parallel to
the gradient. The fact that \Equation{} \ref{eq:differential} holds only locally
(magnitude of $\delta \vc{u}$ must be small) explains why $\eta$ must be a small
number. It should be added that gradient descent can be trapped to (potential)
local minima of the training loss and therefore fail to solve \Equation{}
\ref{eq:problem}. As it will be discussed later, this is not a problem, because
\emph{the ultimate purpose is to find a hypothesis that generalizes well, not
necessarily the one that minimizes the training loss}\footnote{Remember we use
the training loss (see \Equation{} \ref{eq:training_loss}) as a proxy for the
generalization loss (see \Equation{} \ref{eq:generalization_loss}).}. Optimizers
are discussed in further detail in \Section{} \ref{subsec:nn_training}.

Before moving on, it is worth to add that both the regularization and the
optimizer have an effect on the ``true'' or \emph{\textbf{effective
capacity}}\index{Effective capacity} \parencite{deeplearning} of the learner,
\emph{which might be less than the representational
capacity\index{Representational capacity} of the hypothesis space}. For example
a regularizer penalizes the complexity of an hypothesis, effectively
``shrinking'' the representational capacity of the hypothesis space. The effect
of the optimizer can be understood by looking on its contribution to the
solution of \Equation{} \ref{eq:problem}. As described previously, the optimizer
searches through the hypothesis space. If this ``journey'' is not long enough,
then this ``journey'' is practically equivalent to a long ``journey'' in a
shortened version of the original hypothesis space. In the rest of this chapter,
by the term \emph{\textbf{complexity}\index{Complexity} or
capacity\index{Capacity} of a learner, we mean its effective capacity, which is
affected by all its three components}.

\subsection{Performance, complexity and experience}

Suppose that we have trained our learner, and finally we get our model, as
stated by \Equation{} \ref{eq:learner}. \emph{How can we assess its
performance\index{Performance}?} Remember, we can't calculate the generalization
loss\index{Generalization loss}, since we are not given an infinite amount of
data. First of all, \emph{we should not report the training loss\index{Training
loss}, because it is optimistically biased\index{Optimistically biased}}, as it
is evaluated on the same data that has been trained on\footnote{Intuitively,
this is like assessing students' performance based on problems they have already
seen before. They can easily achieve zero error, just by recalling their
memory.}. What we should is to collect new input-output pairs, forming the
\emph{\textbf{test set}}\index{Test set} $\dtest$, and then \emph{estimate the
generalization loss} as following:
\begin{equation}
	\label{eq:test_loss}
	\loss_\text{test} \coloneqq \frac{1}{\lvert \dtest \rvert} \sum_{i \in \dtest}
	\ell(h(\vcx_i), \vcy_i)
\end{equation}
The \emph{test error or loss}\index{Test error}\index{Test loss} is evaluated on
new---unseen to the learner during the training phase\index{Training
phase}---samples and as such, it is an \emph{unbiased estimate}\index{Unbiased
estimate} of the generalization loss. Usually, since many times is not even
possible to collect new samples, \emph{we split the initial
dataset\index{Dataset} into training and test sets}.

The general recipe for building and evaluating the performance of a \gls{ml}
model has already been presented. What is missing is how we can improve its
performance, or to put it differently, the factors that affect the quality of
the returned model. There are two main factors that determine the performance of
the model: \emph{complexity\index{Complexity} and experience\index{Experience}}.
In general, the larger the experience---the training set---\index{Training set}
the better the performance, just like we humans perform improve on a task by
keep practicing. With regards to the complexity, \emph{learners of low
complexity\index{Low complexity} might fail to capture the patterns in the
data}, meaning that the resulting model will fail to generalize. In contrast,
\emph{learners of higher complexity would be able to capture these patterns},
and as such return models of higher quality. However, \emph{as the complexity of
the learner keeps increasing, the latter is more sensitive to noise, i.e. there
is a higher chance that the learner will simply memorize its experience} and as
such, fail to generalize.

In other words, there is a trade-off between the complexity of the learner and
its performance. The learner should be not too simple but also not too complex,
in order to generalize well. This in turn implies that we need to find a way to
``tune'' the complexity. A common way to achieve that is by using another set of
instances, known as the \emph{\textbf{validation set}\index{Validation set}}. We
train learners of different complexity on the training set\index{Training set},
evaluate their performance on the validation set, and then choose the learner
that performs best on the validation set. The reason we use the validation set
instead of the test set for tuning complexity, is to ensure that the performance
estimation is unbiased. \emph{The test set\index{Test set} should not influence
our decisions in any way}. After we have tuned the complexity, we can estimate
the performance of the resulting model in the test set. Finally, it is a good
practice to retrain the learner on the whole dataset---including validation and
test sets---since more data result in models of higher quality.

\begin{theorem}[%
		Bias-variance decomposition\index{Bias-variance decomposition},
		\cite{Bishop2006}%
	]
	\label{thrm:bias_variance}
	From \Equation{} \ref{eq:supervised} and under the assumption that $\epsilon
	\sim \mcl{N}(0, 1)$, the expected squared loss\index{Squared
	loss}\index{Expected square loss} at $\vcx^*$ can be decomposed as
	following:
	\begin{equation}
		\label{eq:bias_variance}
		\exp{\left(y^* - \hat{f}(\vcx^*)\right)^2}
		=
		\color{blue}\left(f(\vcx^*) - \exp{\hat{f}(\vcx^*)}\right)^2
		\color{black}
		+
		\color{red}\exp{\left(\hat{f}(\vcx^*) - \exp{\hat{f}(\vcx^*)}\right)^2}
		\color{black}
		+
		\sigma_{\epsilon}^{2}
	\end{equation}
	The expected squared loss refers to the average squared loss we would obtain
	by repeatedly estimating $f$ using different training sets, each tested at
	$\vcx^*$. The overall expected squared loss can be computed by averaging the
	left hand side of \Equation{} \ref{eq:bias_variance} over all possible values
	$\vcx^*$ in the test set.
\end{theorem}

The trade-off between the complexity\index{Complexity} of the learner and its
performance\index{Performance}, is mathematically described in \Theorem{}
\ref{thrm:bias_variance}. \Equation{} \ref{eq:bias_variance} states that the error
of the learner can be decomposed into three terms:
\emph{\textbf{bias}\index{Bias}, \textbf{variance}\index{Variance} and
\textbf{irreducible error}\index{Irreducible error}}. The bias\index{Bias}
(squared)---first term of \Equation{} \ref{eq:bias_variance}---refers to the error
introduced by \emph{approximating a real-world problem, which can be highly
complicated, by a much simpler model} \parencite{introstat, Hastie2009}. For
instance, if the input-output relationship is highly nonlinearity, using linear
regression to approximate $f$, will undoubtedly introduce some bias in the
estimate of $f$. In contrast, if the input-output relationship is very close to
linear, linear regression should be able to produce an accurate estimate of $f$.
In general, more flexible learners, result in less bias
\parencite{introstat, Hastie2009}.

The variance\index{Variance}---second term of \Equation{}
\ref{eq:bias_variance}---refers to the \emph{degree by which $\hat{f}$ would
change if it was estimated by different training sets\index{Training set}}.
Since the training data\index{Training data} are used to fit the learning
algorithm, different training sets will result in a different estimate of $f$.
Ideally, $\hat{f}$ should not exhibit too much variation between different
training sets, since otherwise small changes in the training can result in large
changes in $\hat{f}$. In that case, the learner essentially memorizes the
training data. Generally, more flexible learners, result in higher
variance\index{Variance} \parencite{introstat, Hastie2009}.

Lastly, the irreducible error---third term of \Equation{}
\ref{eq:bias_variance}---refers to the \emph{error caused by stochastic label
noise}, as can be seen from \Equation{} \ref{eq:supervised}. A possible source for
this noise, might be ommited features\index{Feature} which are useful in
predicting the output. It is called irreducible, because no matter how well we
estimate $f$, even if we predict $\hat{y} = f(\vcx)$, we can't reduce the error
associated to the variability of $\epsilon$. As stated in \Section{}
\ref{subsec:supervised_learning}, this random error term is indepedent of
$\vcx$, and as such, we have no control over it. The \emph{bias-variance
trade-off}\index{Bias-variance trade-off} is schematically depicted in \Figure{}
\ref{fig:bias_variance}. Interested readers might also appreciate reading about
\emph{double descent}\index{Double descent} \parencite{Nakkiran2019}, a
phenomenon where increasing further the complexity of the learner, results in a
new minimum (hence, the name).

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				width=0.5\textwidth, custom axis, domain=0:2,
				grid=none, ticks=none, samples=50, ymin=0, ymax=1.8,
				xlabel=Complexity, ylabel=Error,
			]
			\addplot+ [no markers] {1/(x+0.3)-0.2}; % Bias^2
			\addlegendentry{Bias$^2$};
			\addplot+ [no markers] {0.12*e^(1.40*x)}; % Variance
			\addlegendentry{Variance};
			\addplot+ [green, no markers] {3*(x-2)*x+3.8}; % Total error
			\addlegendentry{Total error};
			\draw[dotted, -Diamond] (1, 0) -- (1, 0.83) node[
				above, yshift=0.2cm
			] {}; %{$\argmin_{\Omega} \mcl{E}$};
		\end{axis}
	\end{tikzpicture}
	\caption[The bias-variance trade-off.]{The bias-variance
	trade-off\index{Bias-variance trade-off}. For a given task\index{Task},
	there is a ``sweetspot'' of complexity, that minimizes the total error.
	$\text{Bias}^2$ and variance correspond to the first and second term of
	\Equation{} \ref{eq:bias_variance}, respectively.}
	\label{fig:bias_variance}
\end{figure}

\Figure{} \ref{fig:learning_curves_theory}, shows the \emph{learning
curves}\index{Learning curve} for learners of different complexity. A learning
curve is a plot of the training and test performance\index{Training
performance}\index{Test performance}\footnote{Usually, only the test performance
is plotted.} of the learner\index{Learner} as function of its experience.
First, lets look at the learning curve of the low complexity learner. The
accuracy\footnote{By accuracy we mean any performance metric\index{performance
metric} where higher values are better, not necessarily the classification
accuracy\index{Classification accuracy}.} starts out high on the training
set\index{Training set}, since with a small number of samples, the learner can
fit them perfectly. However, by adding more training data, learner's training
accuracy\index{Training accuracy} quickly drops due to learners inflexibility
and inexpressivity. That is, it can't fit the patterns in the training data. On
the other hand, test accuracy\index{Test accuracy} starts out very low, since
with very few training data, it is unlikely that the training set is a good
representation of the underlying distribution\index{Distribution} $p(\vcx,
\vcy)$. In other words, it is unlikely that the learner will experience patterns
in the training data, that will help it to generalize well. By increasing the
training data, test accuracy increases but it never reaches a high value. This
happens due to the learners inability to detect and exploit the patterns in the
training data.  In other words, the learner fails to generalize well not because
its experience is low, but because it is biased. That is, it oversimplifies the
problem and make strong assumptions that do not capture the complexity of the
data.

Now lets consider the learning curve of the high complexity\index{High
complexity} learner. Again, the training accuracy starts out high with a small
amount of training data.  However, in contrast to the previous case, as the
number of training samples increases, the training accuracy remains high since
the learner is flexible enough to learn the patterns in the training set,
irrespective of its size. At some point, the training data becomes large enough
that is a good representation of the underlying distribution\index{Distribution}
$p(\vcx, \vcy)$ and since the learner is very flexible, it can capture the true
patterns in $p(\vcx, \vcy)$, increasing the test accuracy. It is worth pointing
out that the learning and complexity curves\index{Complexity curve} (see \Figure{}
\ref{fig:bias_variance}), are just two slices of the same 3D plot: the plot of
performance as function of experience and complexity.

\begin{figure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\begin{tikzpicture}%[trim axis left, trim axis right]
			\begin{axis}[
					custom axis, no markers, ticks=none,
					xmin=0, xmax=3.5, domain=0:3.5, grid=none,
					ymin=0, ymax=1.2, clip mode=individual,
					xlabel=Number of training samples, ylabel=Accuracy,
				]
				\addplot {divide(25*x, 2+25*x)*0.6};
				\addlegendentry{Test accuracy};
				\addplot {1 - 0.59*divide(25*x, 2+25*x)*0.6};
				\addlegendentry{Training accuracy};
				\addplot+ [black, dashed] {1};
				\node at (-0.29, 1) {100\%};
			\end{axis}
		\end{tikzpicture}
		\caption{Learning curve of learner with low complexity\index{Low
		complexity}.}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\begin{tikzpicture}%[trim axis left, trim axis right]
			\begin{axis}[
					custom axis, no markers, ticks=none,
					xmin=0, xmax=3.5, domain=0:3.5, grid=none,
					ymin=0, ymax=1.2, clip mode=individual,
					xlabel=Number of training samples, ylabel=Accuracy,
				]
				\addplot {divide(25*x, 11+25*x)*0.97};
				\addlegendentry{Test accuracy};
				\addplot {1 - 0.1*divide(25*x, 11+25*x)*0.97};
				\addlegendentry{Training accuracy};
				\addplot+[black, dashed] {1};
				\node at (-0.29, 1) {100\%};
			\end{axis}
		\end{tikzpicture}
		\caption{Learning curve of learner with high complexity\index{High
		complexity}.}
	\end{subfigure}
	\caption{Relation between performance\index{Performance} and
	experience\index{Experience}.}
	\label{fig:learning_curves_theory}
\end{figure}


%\begin{definition}[Overfitting]
%	Add some text!
%\end{definition}
%
%\begin{definition}[Underfitting]
%	Add some text!
%\end{definition}
%
%\begin{theorem}[No free lunch]
%	Add some text!
%\end{theorem}

\section{Fundamentals of Deep Learning}

Having covered the basic jargon and concepts of \gls{ml}, we are now in a
position to dive into \gls{dl}. One might expect that \gls{dl}\index{Deep
learning} is a very complex subfield of \gls{ml}, given its astonishing results
in complex tasks, but quite the opposite holds. Notably, \gls{dl} shares similar
ideas with reticular chemistry: \emph{combining simple computational units,
known as \textbf{neurons}\index{Neuron}, to achieve intelligent
behavior\index{Intelligent behavior}}. And just like we can tune the properties
of \glspl{mof} by judiciously selecting and combining their building blocks, we
can design problem-specific \emph{{neural
\textbf{architectures}}}\index{Architecture} by reasonably arranging and
connecting the neurons. In other words, both \gls{dl} and reticular
chemistry\index{Reticular chemistry} can be viewed as building with Legos.

Since the term ``neuron'' is admittedly a neuroscience\index{Neuroscience} term,
one might wondering what is the relation between \gls{dl}\index{Deep learning}
and the human brain. The neural perspective on \gls{dl} is mainly motivated by
the following idea: \emph{the brain is a proof by example that intelligent
behavior\index{Intelligent behavior} is possible and as such, a straightforward
approach to build an intelligent system\index{Intelligent system} is by reverse
engineering the computational principles behind the brain and duplicating its
functionality}. However, the term ``deep learning'' is not limited to this
neuroscientific perspective. It appeals to a more general principle of learning
\emph{multiple levels of abstraction}, which is applicable in \gls{ml}
frameworks that are not necessary neurally inspired \parencite{deeplearning}.

\begin{definition}[Deep learning]
	Class of machine learning algorithms inspired by brain organization, based
	on learning multiple levels of representation and abstraction. They achieve
	great power by learning to represent the world\footnote{Hierarchy is deeply
	rooted in our world. Just think the hierarchy from subatomic
	particles\index{Subatomic particles} to macroscopic
	objects\index{Macroscopic objects}.} as a nested hierarchy of concepts.
\end{definition}

Before exploring \glspl{nn} we first need to understand how the neuron, the
basic building block of \glspl{nn}, works.  \emph{A neuron is nothing more than
a device---\textbf{a simple computational unit}---that makes decisions by weighing up
evidence} \parencite{Nielsen2018}. This sounds very similar to the way humans
make decisions. For instance, suppose the weekend is coming up and your favorite
singer has scheduled a concert near your city. In order to decide whether you
should go to the concert or not, you weigh up different factors, such as weather
conditions, ease of transportation (you don't own a car) and whether your
boyfriend or girlfriend is willing to accompany you. This kind of
decision-making can be described mathematically as following:
\begin{equation}
	\label{eq:perceptron}
	\text{decision}
	=
	\begin{cases}
		1 \quad \vc{b}^\top \vcx + \beta_0 > 0 \\
		0 \quad \vc{b}^\top \vcx + \beta_0 \leq 0 \\
	\end{cases}
	\quad
	\text{where}
	\quad
	\vc{b}^\top\vcx \coloneqq \sum_i \beta_i x_i
\end{equation}
If this weighted sum\index{Weighted sum} plus the bias\index{Bias}\footnote{If
you prefer the neuroscientific analogy, you can think of bias as how easy is for
a neuron\index{Neuron} to ``fire''.}---your willing to to go to the festival
irrespective of the evidence---is greater than zero, then your decision is
positive, otherwise negative.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			on grid, node distance=1.4cm and 1.8cm, thick,
		]
		\node[custom node] (x) {$\vc{x}$};
		\node[custom node, above right=of x] (w) {$\vc{\beta}$};
		\node[custom node, right=of x] (dot) {\Huge$\cdot$}
			edge[<-] (x) edge[<-] (w);
		\node[custom node, below right=of x] (b) {$\beta_0$};
		\node[
			custom node, right=of dot, circle split,
			rotate=90, inner sep=2pt,
			pin={[pin edge={->, black, thick}]below:$y$},
		] (y) {
				$+$
				\nodepart{lower}
				\rotatebox{-90}{$\rfloor\hspace{-4.4pt}\lceil$}
			} edge[<-] (dot) edge[<-] (b);
	\end{tikzpicture}
	\caption{The perceptron.}
	\label{fig:perceptron}
\end{figure}

The simple decision-making rule specified by \Equation{} \ref{eq:perceptron}, which
is known as the \emph{\textbf{perceptron}}\index{Perceptron}
\parencite{Rosenblatt1957}, is schematically depicted in \Figure{}
\ref{fig:perceptron}. Essentially, the perceptron is a linear binary
classifier\index{Binary classifier}\index{Linear binary classifier} (see also
\Figure{} \ref{fig:classification}). If we pay a little more attention to \Equation{}
\ref{eq:perceptron}, we can see that that the decision is basically an
\emph{application of a linear function\index{Linear function}}\footnote{Formally
speaking it is an affine function\index{Function}. We can turn it into a linear
by ``absorbing'' the bias term into the weights and adding $1$ to the input
vector, a procedure known as the \emph{bias trick}\index{Bias trick}.}
\emph{followed by a nonlinearityity\index{Non-linearity}}. As such, we can rewrite
\Equation{} \ref{eq:perceptron} as following:
\begin{equation}
	\label{eq:neuron}
	y = \phi\left(\vc{\beta}^\top \vcx + \beta_0\right)
\end{equation}
where $\phi(\cdot)$ is the nonlinear function aka \emph{\textbf{activation
function}}\index{Activation function}.

In the perceptron, the activation function is the Heavyside step
function\index{Heavyside step function} but in modern \glspl{nn} it has
substituted by functions such as the sigmoid\index{Sigmoid}, hyperbolic
tangent\index{Hyperbolic tangent} and currently the \gls{relu}\index{\gls{relu}}
and its variants. Some activation functions are graphically shown in \Figure{}
\ref{fig:activation_functions}. The reason that the step function isn't used
anymore is that its derivative\index{Derivative} vanishes everywhere, which is
problematic for gradient-based optimization methods \index{Gradient-based
optimization} that power the training of modern \glspl{nn}. The \gls{relu}
function is defined as:
\begin{equation}
	\relu(x) \coloneqq
	\max(0, x) =
	\begin{cases}
		x \quad x > 0 \\
		0 \quad \text{otherwise}
	\end{cases}
\end{equation}
A common variant of \gls{relu} is the \gls{lrelu}\index{\gls{lrelu}}
function which is defined as:
\begin{align}
	\lrelu(x) \coloneqq
	\max(0, x) + a\min(0, x)=
	\begin{cases}
		x \quad & x > 0 \\
		ax \quad & \text{otherwise} \\
	\end{cases}
\end{align}
where $a$ is a small positive constant usually set to \num{0.01}. It is worth to
notice how simple the nonlinearities used in \gls{nn} are. For instance,
\gls{relu}, the most commonly used activation function these days, is just a
piecewise linear function\index{Piecewise linear function}. This again
highlights the fundamental idea behind \gls{dl}: \emph{building something
complex by combining simple elements}.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
				no marks, domain=-4:4, trim axis left, trim axis right,
				ymin=0, ymax=1, thick, legend columns=-1, enlargelimits=true,
				legend style={at={(0.5, 1)}, anchor=south},
				width=0.5\textwidth
			]
			\addplot coordinates {(-4, 0) (0, 0) (0, 1) (4, 1)};
			\addlegendentry{$\step(\cdot)$};
			\addplot {divide(1, 1 + exp(-x))};
			\addlegendentry{$\sigma(\cdot)$};
			\addplot+[green] {max(0, x)};
			\addlegendentry{$\relu(\cdot)$};
		\end{axis}
	\end{tikzpicture}
	\caption{Examples of activation functions.}
	\label{fig:activation_functions}
\end{figure}

\subsection{Neural networks}

\Acrlongpl{nn} can be thought as \emph{\textbf{collection of neurons} organized
in layers} and can be represented as \emph{computational
graphs.}\index{Computational graph}\index{Graph}.

\begin{definition}[Graph]
	A set of objects in which some pairs of objects are in some sense
	``related''. See \Figure{} \ref{fig:graphs} for some types of graphs.
\end{definition}

\begin{definition}[Computational graph]
	A \gls{dag} where nodes correspond to operations or variables and edges show
	the data flow between the nodes.
\end{definition}

\noindent A typical architecture known as \gls{mlp}\index{Multilayer
perceptron}\footnote{The term \gls{mlp} is kind of a misnomer, since the step
function used originally in the perceptron is no longer used in modern
\glspl{nn}.} or \gls{fcnn}\index{Fully connected neural network} is presented in
\Figure{} \ref{fig:mlp}. In general, the architecture\index{Architecture} of a
\gls{nn} can be broken down into the following three layers: \emph{\textbf{input
layer}\index{Input layer}, \textbf{hidden layer}\index{Hidden layer} and
\textbf{output layer}\index{Output layer}}. Information flow starts from the
input layer, passes through the hidden layer(s) and finally ends at the output
layer. \Acrlongpl{nn} with more than one hidden layer are classified as deep, and
shallow otherwise. It should be emphasized that all the neurons in the hidden
layers aka \emph{hidden units}\index{Hidden unit} perform exactly the same
operation as that of the perceptron, described in \Equation{} \ref{eq:neuron}. As
such all the \emph{hidden units at a given layer make decisions based on the
decisions of the previous layer}. Unsurprisingly, the kind of decisions made by
the neurons depends solely on the problem and the data
distribution\index{Distribution} at hand.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			input/.style={
				circular drop shadow, circle, fill=blue!20, draw=blue, thick
			},
			hidden/.style={
				circular drop shadow, circle, fill=orange!20, draw=orange, thick
			},
			output/.style={
				circular drop shadow, circle, fill=green!20, draw=green, thick
			},
			new set=hid1, new set=hid2, new set=hid3, new set=hid4,
		]
		\foreach \l in {1, 2, 3, 4}
			\foreach \n in {1, 2, 3, 4} {
				\node[hidden, set=hid\l] at (\l, \n) {};
			}
		\node at (2.5, 4.5) {Hidden layer};
		\node at (2.5, 0.5) {Feature extraction};
		\node[
			input, label=left:\shortstack{Input \\ layer}
		] at (0, 2.5) (inp) {$\vcx$};
		\node[
			output, label=right:\shortstack{Output \\ layer}
		] at (5, 2.5) (out) {$\vcy$};
		\graph[
			grow right sep=1cm,
		] {
			(inp) -> (hid1)
			-> [complete bipartite] (hid2)
			-> [complete bipartite] (hid3)
			-> [complete bipartite] (hid4)
			-> (out)
		};
	\end{tikzpicture}
	\caption[The multilayer perceptron.]{The multilayer perceptron. A typical
	example of a \acrlong{nn}.}
	\label{fig:mlp}
\end{figure}

\begin{figure}
	\begin{subfigure}[t]{0.245\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.5cm, on grid, semithick]
			\node[custom node] (1) {1};
			\node[custom node, right of=1] (2) {2};
			\node[custom node, above of=1] (3) {3}
				edge (1);
			\node[custom node, above of=2] (4) {4}
				edge (2) edge (3);
			\node[custom node, above of=3, xshift=0.75cm] {5}
				edge (3) edge (4);
		\end{tikzpicture}
		\caption{Undirected graph.}
	\end{subfigure}
	\begin{subfigure}[t]{0.245\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.5cm, on grid, semithick]
			\node[custom node] (1) {1};
			\node[custom node, right of=1] (2) {2};
			\node[custom node, above of=1] (3) {3}
				edge [<-] (1);
			\node[custom node, above of=2] (4) {4}
				edge [<-] (2) edge [red, <-] (3);
			\node[custom node, above of=3, xshift=0.75cm] {5}
				edge [red, ->] (3) edge [red, <-] (4);
		\end{tikzpicture}
		\caption{Directed cyclic graph.}
	\end{subfigure}
	\begin{subfigure}[t]{0.245\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.5cm, on grid, semithick]
			\node[custom node] (1) {1};
			\node[custom node, right of=1] (2) {2};
			\node[custom node, above of=1] (3) {3}
				edge [<-] (1);
			\node[custom node, above of=2] (4) {4}
				edge [<-] (2) edge [blue, <-] (3);
			\node[custom node, above of=3, xshift=0.75cm] {5}
				edge [blue, <-] (3) edge [blue, <-] (4);
		\end{tikzpicture}
		\caption{\Acrlong{dag}.}
	\end{subfigure}
	\begin{subfigure}[t]{0.245\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.5cm, on grid, semithick]
			\node[custom node] (x1) {$a$};
			\node[custom node, right of=x1] (x2) {$b$};
			\node[custom node, above of=x1] (h1) {$*$}
				edge[<-] (x1) edge[<-] (x2);
			\node[custom node, above of=x2] (h2) {$-$}
				edge [<-] (x1) edge[<-] (x2);
			\node[custom node, above of=h1, xshift=0.75cm] {$+$}
				edge [<-] (h1) edge [<-] (h2);
		\end{tikzpicture}
		\caption{Computational graph.}
	\end{subfigure}
	\caption{Examples of graphs. In a directed graph\index{Directed graph} the
	edges have direction. If they contain at least one loop are called cyclic,
	otherwise acyclic.}
	\label{fig:graphs}
\end{figure}

To understand better the purpose of the hidden layers and the functionality of a
\gls{nn} as a whole, lets consider the problem of image
classification\index{Image classification}. This is by no means a trivial task,
since we need to learn a mapping from a set of pixels to an object identity.
Imagine for a moment you are blindfolded and you need to classify an image. The
valid classes are: ``person'', ``car'' and ``ship''. Furthermore, assume that
the correct class is ``person''. \emph{Would you prefer to know the
sequence of pixels values or whether the image contains a face?} In other words,
it is a lot easier to classify the content of an image if we know some
\emph{high-level features}\index{High-level feature}. \emph{\Acrlongpl{nn}
extract such high level features by exploiting the hierarchical structure of an
image}. A complex object like a ``face'' is defined in term of simpler ones,
such as ``eyes and ``nose'', which in turn are defined in terms of simpler ones
and so on. This hierarchy allows the \gls{nn} to solve the complex task of image
classification by breaking it down into smaller sub-problems. The first layer
learns to detect edges. The second layer combines the decisions of the first
layer to detect corners. Subsequently, the third layer combines the decisions of
the second layer to identify shapes like circles and squares and so on, until we
reach the final layer which is able to detect high level features such as
objects or object parts. The deeper we are into the network---i.e. the closer to
the output layer\index{Output layer}---the more abstract and task-specific the
detected features become.

In a \gls{fcnn} with $n$ hidden layers, each hidden layer performs the following
operation:
\begin{equation}
	\vc{h}^{t} =
	\vc{\phi}\left({W^t\vc{h}^{t-1} + \vc{\beta}_0}\right)
	\quad
	\text{where}
	\quad
	1 \leq t \leq n
	\quad
	\text{and}
	\quad
	\vc{h}^0 \coloneqq \vcx
\end{equation}
which is just a matrix version of \Equation{} \ref{eq:neuron} with the matrix $W^t$
playing the role of the ``synapses''\index{Synapses} between the
neurons\index{Neuron} of the layers $t-1$ and $t$. Since the ``stacking'' of
many hidden layers is equivalent to a huge composite function\index{Composite
function}\begin{equation}
	\vc{\tau}(\vcx) \coloneqq
	\left(\vc{h}^t \circ \vc{h}^{t-1}
	\cdots
	\circ \vc{h}^{1}\right)(\vcx)
\end{equation}
and the output layer is just a linear function of the last hidden
layer, the output\index{Output} of the \gls{fcnn} can be written as:
\begin{equation}
	\hat{y} =
	\vc{\beta}^\top \vc{\tau}(\vcx) + \beta_0
\end{equation}
or in the general case of multi-valued output\index{Multi-valued output}:
\begin{equation}
	\label{eq:mlp}
	\hat{\vc{y}} =
	W\vc{\tau}(\vcx) + \vc{\beta}_0
\end{equation}
In other words, \emph{a linear model\index{Linear model} on top of the extracted
features\index{Extracted features}}. Moreover, the use of activation function
now becomes more clear: \emph{the composition of many linear functions is
just another linear function, which implies nonlinearities must be inserted
between them, if we aim to learn a nonlinear relationship}. \Equation{}
\ref{eq:mlp} can also be understood in the following way: \emph{a problem that
is nonlinear---i.e. complex---in the original space, can become linear---i.e.
simple---in a transformed space}. \Figure{} \ref{fig:xor} shows such an example,
known as the XOR problem\index{XOR problem}. The solution essentially boils down
to finding the right transformation funciton $\vc{\tau}(\vcx)$. Traditional
\gls{ml} algorithms like \glspl{svm} tackle while \gls{dl} takes a compl

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\begin{tikzpicture}
			\begin{axis}[
					width=\textwidth, xtick={0, 1}, ytick={0, 1},
					xlabel=$x_1 \phantom{h_1}$, ylabel=$x_2$,
					title=Original space, only marks,
				]
				\addplot+[mark=text, text mark=$\mathbf{0}$]coordinates {(0, 0) (1, 1)};
				\addplot+[mark=text, text mark=$\mathbf{1}$] coordinates {(0, 1) (1, 0)};
			\end{axis}
		\end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
		\begin{tikzpicture}
			\begin{axis}[
					width=\textwidth, xlabel=$h_1$, ylabel=$h_2$,
					title=Transformed space, only marks,
					xtick={0, 1, 2}, ytick={0, 1, 2}
				]
				\addplot+[mark=text, text mark=$\mathbf{0}$] coordinates {(0, 0) (2, 1)};
				\addplot+[mark=text, text mark=$\mathbf{1}$] coordinates {(1, 0)};
			\end{axis}
		\end{tikzpicture}
	\end{subfigure}
	\caption[Solving the \texttt{XOR} problem.]{Solving the \texttt{XOR}
	problem. A linear classifier in the original space can't perfectly seperate
	the ``ones'' and ``zeros''. In contrast, if the points are projected into a
	new space, then they become linearly separable\index{Linearly separable}.}
	\label{fig:xor}
\end{figure}


%\begin{theorem}[Universal approximation theorem]
%	Fill the content of the theorem.
%\end{theorem}
%
%\begin{theorem}[No free lunch theorem]
%	Fill the content of the theorem.
%\end{theorem}

%\subsection{Convolutional neural networks}
%
%\subsection{Regularizing neural networks}
%
%\subsection{Training neural networks}
%\label{subsec:nn_training}

%\begin{algorithm}[H]
%	\KwIn{
%		$\dtrain$, loss function $\ell$,
%		model parameters $\vcth$, learning rate
%		$\eta$, batch size $\lvert\mcl{B}\rvert$
%	}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\mcl{B} \gets$ sample $\lvert\mcl{B}\rvert$ datapoints from $\dtrain$\;
%		$\gloss \gets \frac{1}{\lvert\mcl{B}\rvert} \sum_{i \in \mcl{B}}
%		\nabla_{\vcth}\ell_i(\vcth)$\;
%		$\vcth \gets \vcth - \eta\gloss$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[Batch, mini-batch and stochastic gradient
%	descent]{Batch\index{Batch gradient descent}, mini-batch\index{Mini-batch
%	gradient descent} and stochastic gradient descent\index{Stochastic gradient
%	descent} \parencite{Bottou1998}}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{Model parameters $\vcth$, momentum $\beta$, learning rate $\eta$}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\vc{m} \gets \beta\vc{m} - \gloss$\;
%		$\vcth \gets \vcth + \vc{m}$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[Momentum]{Momentum\index{Momentum} \parencite{Polyak1964}}
%	\label{algo:momentum}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{Model parameters $\vcth$, momentum $\beta$, learning rate $\eta$}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\vc{m} \gets \beta\vc{m} -
%		\eta\nabla_{\vcth}\loss(\vcth + \beta\vc{m})$\;
%		$\vcth \gets \vcth + \vc{m}$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[Nesterov momentum]{Nesterov momentum\index{Nesterov momentum}
%	\parencite{Sutskever2013}}
%	\label{algo:nesterov}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{
%		Model parameters $\vcth$, momentum $\beta$, learning rate $\eta$,
%		smoothing term $\epsilon$
%	}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\vc{s} \gets \vc{s} + \gloss \odot \gloss$\;
%		$\vcth \gets \vcth - \eta\gloss \oslash \sqrt{\vc{s} + \epsilon}$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[AdaGrad]{AdaGrad\index{AdaGrad} \parencite{Duchi2011}}
%	\label{algo:adagrad}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{
%		Model parameters $\vcth$, decay rate $\beta$, learning rate $\eta$,
%		smoothing term $\epsilon$
%	}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\vc{s} \gets \vc{s} + (1 - \beta) \gloss \odot \gloss$\;
%		$\vcth \gets \vcth - \eta\gloss \oslash \sqrt{\vc{s} + \epsilon}$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[RMSProp]{RMSProp\index{RMSProp} \parencite{Tieleman2012}}
%	\label{algo:rmsprop}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{
%		Model parameters $\vcth$,
%		learning rate $\eta$, smoothing term $\epsilon$,
%		momentum decay $\beta_1$, scaling decay $\beta_2$
%	}
%	%\KwOut{Optimized parameters $\vcth$}
%	\BlankLine
%	$\vcth \gets$ random initialization\;
%	\While{stopping criterion not met}{
%		$\vc{m} \gets \beta_1\vc{m} - (1 - \beta_1) \gloss$\;
%		$\vc{s} \gets \beta_2\vc{s} + (1 - \beta_2) \gloss \odot \gloss$\;
%		$\vc{\widehat{m}} \gets \frac{\vc{m}}{1 - \beta_1^t}$\;
%		$\vc{\widehat{s}} \gets \frac{\vc{s}}{1 - \beta_2^t}$\;
%		$\vcth \gets \vcth + \eta\widehat{\vc{m}}
%		\oslash \sqrt{\vc{\widehat{s}} + \epsilon}$\;
%	}
%	\Return{optimized parameters $\vcth$}
%	\caption[Adam]{Adam\index{Adam} \parencite{Kingma2017}}
%	\label{algo:adam}
%\end{algorithm}
%
%\begin{algorithm}[H]
%	\KwIn{Computational graph $\mcl{G}$ where nodes $u_i$ follow a topological
%	ordering\footnote{Any ordering such that parents come before children.}}
%	\BlankLine
%	\tcc{Forward pass}
%	\For{$i=1$ \KwTo $N$}{
%		Compute $u_i$ as a function of $\text{Pa}_\mcl{G}(u_i)$\;
%	}
%	$u_N = 1$\;
%	\tcc{Backward pass}
%	\For{$i=N-1$ \KwTo $1$}{
%		$\bar{u}_i =
%		\sum_{j \in \text{Ch}_\mcl{G}(u_i)} \bar{u}_j \pd{u_j}{u_i}$\;
%	}
%	\Return{derivatives $\bar{u}_i$}
%	\caption[Backpropagation]{Backpropagation\index{Backpropagation}
%	\parencite{Rumelhart1986}}
%	\label{algo:backpropagation}
%\end{algorithm}
%
%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{momentum.pdf}
%\end{figure}
%
%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{nesterov.pdf}
%\end{figure}

%\begin{figure}
%	\centering
%	\begin{tikzpicture}
%		\pgfplotsset{width=0.8\textwidth}
%		\begin{axis}[
%				clip mode=individual, custom axis, domain=-8:8, ymin=0, ymax=10,
%				no markers, xlabel=$w$, ylabel=$\mathcal{L}(w)$, samples=500, grid=none
%			]
%			\draw[gray, fill=gray!30] (-2, 0) rectangle (2, 10);
%			\addplot {pow(x-2, 2)};
%			\addlegendentry{$\mathcal{L}_1$};
%			\addplot {pow(x, 2)};
%			\addlegendentry{$\mathcal{L}_2$};
%			\addplot {pow(x+2, 2)};
%			\addlegendentry{$\mathcal{L}_3$};
%			\addplot {pow(3*x, 2) + 2.667};
%			\addlegendentry{$\mathcal{L}_{\mathrm{T}}$};
%			%\node at (-2, -0.9) [font=\scriptsize] {$\min_i w_i$};
%			\node at (-2, -0.9)  {$\min_i w_i$};
%			%\node at (2, -0.9) [font=\scriptsize] {$\max_i w_i$};
%			\node at (2, -0.9)  {$\max_i w_i$};
%			\node at (0, 2.4) {$w^*$};
%			%\draw[dotted, -Diamond] (0, 0) -- (0, 2.1) node[above] {$w^*$};
%		\end{axis}
%	\end{tikzpicture}
%	\caption{Explaining why SGD works.}
%\end{figure}

%\begin{itemize}
%	\item Universal approximation theorem and no free-lunch theorem
%\end{itemize}
